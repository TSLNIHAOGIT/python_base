{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path F:\\陶士来文件\\tsl_python_project\\titanic_disaster\n"
     ]
    }
   ],
   "source": [
    "print('current path',os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  \n0                            Braund, Mr. Owen Harris    male  22.0  \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0  \n2                             Heikkinen, Miss. Laina  female  26.0  \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0  \n4                           Allen, Mr. William Henry    male  35.0  \n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(r'F:\\陶士来文件\\tsl_python_project\\python_base\\pandas\\train.csv',\n",
    "               usecols=[ 'Name','Sex','Age', 'PassengerId',  'Survived',  'Pclass'])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age  maxValue  minValue  filterCount\n0  0.42       804       804            1\n1  0.67       756       756            1\n2  0.75       645       470            2\n3  0.83       832        79            2\n4  0.92       306       306            1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\陶士来文件\\software\\python3_6_5\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  \n"
     ]
    }
   ],
   "source": [
    "###groupby的几种使用形式，以后补充\n",
    "\n",
    "get_count = lambda x:x.count()##原例以后补充\n",
    "data_new=data['PassengerId'].groupby(data['Age']).agg({'maxValue':'max','minValue':'min','filterCount':get_count}).reset_index()\n",
    "#add_suffix(suffix)，add_prefix(prefix)添加前缀和后缀\n",
    "print(data_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么要用agg：\n",
    "# 如果你不用agg，也可以实现同样的功能，因为Pandas自带了max、min、mean这些聚合函数，用法如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1='PassengerId'\n",
    "col2='Age'\n",
    "\n",
    "data00 = data[col1].groupby(data[col2]).max()\n",
    "data11 = data[col1].groupby(data[col2]).mean()\n",
    "data22 = data[col1].groupby(data[col2]).min()\n",
    "# 但是这样既笨重，又没办法通过自定义的聚合函数来实现自定义统计，\n",
    "# 而用agg可以传入自定义的聚合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age\n0.42    804\n0.67    756\n0.75    645\n0.83    832\n0.92    306\nName: PassengerId, dtype: int64 Age\n0.42    804.0\n0.67    756.0\n0.75    557.5\n0.83    455.5\n0.92    306.0\nName: PassengerId, dtype: float64 Age\n0.42    804\n0.67    756\n0.75    470\n0.83     79\n0.92    306\nName: PassengerId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data00.head(),data11.head(),data22.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么agg里面要放字典\n",
    "# agg里面可以不传字典，但是新生成的列是不能直接生成你想要的列名的。如果你想要定义列名，那你得这么做：\n",
    "#先初始化列为1，再给这一列传入统计后的值\n",
    "\n",
    "data['max'] = 1\n",
    "data['max'] = data[col1].groupby(data[col2]).agg('max').reset_index()\n",
    "print(data)\n",
    "add_prefix()\n",
    "# 用这种方法，如果同时实现最大值、最小值、均值、计数等参数统计的话，需要写八行代码，\n",
    "# 而在agg中传入字典参数，同时定义列名和所用的聚合函数，只需要一列就能完成了，甚好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid   tran_type\n1001  0            1.0\n      1            2.0\n1002  0            3.0\n1003  0            NaN\n      1            4.5\nName: amount, dtype: float64\ntran_type    0    1\nuid                \n1001       1.0  2.0\n1002       3.0  NaN\n1003       NaN  4.5\ntran_type    0    1\nuid                \n1001       1.0  2.0\n1002       3.0  NaN\n1003       NaN  4.5\n      tran_type_0  tran_type_1\nuid                           \n1001          1.0          2.0\n1002          3.0          NaN\n1003          NaN          4.5\n"
     ]
    }
   ],
   "source": [
    "# 为什么要写reset_index\n",
    "# 这是因为 比如groupby 用户字段，那么 用户字段会默认认定为是index,这并不符合预期，因为往往用户字段应该还原为普通列字段，方便后面和其他含用户字段的表的join或merge\n",
    "# \n",
    "# 如果是分组字段是多个，被分组统计字段是一个(或多个)时，用上面的模板好像有点问题：\n",
    "# 要用到unstack()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame({'uid':[1001,1001,1002,1003,1003,1003],'tran_type':[0,1,0,1,0,1],'amount':[1,2,3,4,np.NaN,5]})\n",
    "\n",
    "\n",
    "print(df.groupby(['uid','tran_type'])['amount'].mean())\n",
    "\n",
    "\n",
    "print(df.groupby(['uid','tran_type'])['amount'].mean().unstack())\n",
    "\t\n",
    "df=df.groupby(['uid','tran_type'])['amount'].mean().unstack()\n",
    "print(df)\n",
    "df.columns=['tran_type_0','tran_type_1']\n",
    "df.reset_index()\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这样的数据差不多是我们想要的了\n",
    "# 但是总感觉有点麻烦，\n",
    "# 能不能用agg的方法，试了一下\n",
    "df=pd.DataFrame({'uid':[1001,1001,1002,1003,1003,1003],'tran_type':[0,1,0,1,0,1],'amount':[1,2,3,4,np.NaN,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mean     \ntran_type    0    1\nuid                \n1001       1.0  2.0\n1002       3.0  NaN\n1003       NaN  4.5\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['uid','tran_type'])['amount'].agg({'mean'}).unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t\t\tmean\n",
    "# tran_type\t0     \t1\n",
    "# uid\t\t\n",
    "# 1001\t   1.0      2.0\n",
    "# 1002\t   3.0     NaN\n",
    "# 1003\t  NaN\t4.5\n",
    "# 牵扯到了多级索引了\n",
    "# \n",
    "# 如何转化为大致 \n",
    "#    \tuid\ttran_type_0\ttran_type_1\n",
    "#    0\t1001\t1.0\t        2.0\n",
    "#    1\t1002\t3.0\t        NaN\n",
    "#    2\t1003\tNaN\t4.5\n",
    "#    的样子呢\n",
    "# --------------------- \n",
    "# 作者：波波虾遁地兽 \n",
    "# 来源：CSDN \n",
    "# 原文：https://blog.csdn.net/zlb872551601/article/details/84800334 \n",
    "# 版权声明：本文为博主原创文章，转载请附上博文链接！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    uid  amount_mean  amount_min\n0  1001          1.5         1.0\n1  1002          3.0         3.0\n2  1003          4.5         4.0\n"
     ]
    }
   ],
   "source": [
    "df=df[['uid','amount']].groupby(['uid']).agg({'mean','min'})\n",
    "# \t\tamount\n",
    "# \tmean\tmin\n",
    "# uid\t\t\n",
    "# 1001\t1.5\t1.0\n",
    "# 1002\t3.0\t3.0\n",
    "# 1003\t4.5\t4.0\n",
    "# 可读性还好，但是这没办法和后面的含有uid的表join或merge吧\n",
    "# 因为是多级索引，要是能降级就好了，\n",
    "df.columns\n",
    "# MultiIndex(levels=[['amount'], ['mean', 'min']],\n",
    "#            labels=[[0, 0], [0, 1]])\n",
    "# 一种想法是直接去掉一级就好了\n",
    "#直接去掉一层索引，df.columns = df.columns.droplevel(0)，这是鸵鸟策略吧\n",
    "#df.columns=['uid','amount_mean','amount_min']\n",
    "#第二种想法手动去写列名，是不是显得不够专业啊，下面大神给出的# 把2层合并到一层的 骚写法，\n",
    "df.columns = [\"_\".join(x) for x in df.columns.ravel()]\n",
    "print(df.reset_index())\n",
    "# uid\tamount_mean\tamount_min\n",
    "# 0\t1001\t1.5\t1.0\n",
    "# 1\t1002\t3.0\t3.0\n",
    "# 2\t1003\t4.5\t4.0\n",
    "# 这样就是我们想要的数据了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    uid  amount_mean_0  amount_mean_1\n0  1001            1.0            2.0\n1  1002            3.0            NaN\n2  1003            NaN            4.5\n"
     ]
    }
   ],
   "source": [
    "# 再次回到我们开始提出的问题，能不能用agg的方法对groupby多个字段，\n",
    "# 同样用到上面的对多级索引降级的方式就可以很好解决 问题了\n",
    "df=pd.DataFrame({'uid':[1001,1001,1002,1003,1003,1003],'tran_type':[0,1,0,1,0,1],'amount':[1,2,3,4,np.NaN,5]})\n",
    "df=df.groupby(['uid','tran_type'])['amount'].agg({'mean'}).add_prefix('amount_').unstack()\n",
    "# \t\tamount_mean\n",
    "# tran_type\t0\t1\n",
    "# uid\t\t\n",
    "# 1001\t 1.0\t     2.0\n",
    "# 1002\t 3.0\t     NaN\n",
    "# 1003\t NaN     4.5\n",
    "# 使用层次索引合并降级\n",
    "df.columns = [x[0]+\"_\"+str(x[1]) for x in df.columns.ravel()]\n",
    "#这里因为0是数值型，直接和字符串amount_mean不能用\"_\".join(x) 方式连接\n",
    "df.columns\n",
    "print(df.reset_index())\n",
    "# \tuid\tamount_mean_0\tamount_mean_1\n",
    "# 0\t1001\t1.0\t                 2.0\n",
    "# 1\t1002\t3.0\t                NaN\n",
    "# 2\t1003\tNaN\t          4.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这正是我们想要的\n",
    "# \n",
    "# 和pandas.groupby常一起用的还有transform函数和describe函数，cut函数，\n",
    "# 以及计数的size()函数(计np.NaN)和count()函数（不计np.NaN）\n",
    "# \n",
    "# describe函数比较简单易懂，常见统计量的summary\n",
    "# transform函数则在于，在原df的索引index不变下方便的在原df中增加groupby之后的聚合结果\n",
    "# \n",
    "# 前面进行聚合运算的时候，得到的结果是一个以分组名为 index 的结果对象。如果我们想使用原数组的 index 的话，\n",
    "# 就需要进行 merge 转换。transform(func, args, *kwargs) 方法简化了这个过程，\n",
    "# 它会把 func 参数应用到所有分组，然后把结果放置到原数组的 index 上（如果结果是一个标量，就进行广播）：\n",
    "# 即transform隐式地帮我们完成了一次merge操作，\n",
    "df['count_B']=df.groupby(['group1', 'group2'])['B'].transform('count')\n",
    "#1\n",
    "# 上面运算的结果分析： {‘group1’:’A’, ‘group2’:’C’}的组合共出现3次，即index为0,1,2。对应”B”列的值分别是\n",
    "# ”one”,”NaN”,”NaN”，由于count()计数时不包括Nan值，因此{‘group1’:’A’, ‘group2’:’C’}的count计数值为1。\n",
    "# transform()方法会将该计数值在dataframe中所有涉及的rows都显示出来（我理解应该就进行广播）\n",
    "# \n",
    "# 对连续数值变化的列做分组，会用到cut函数，cut函数中需自定义bins的范围\n",
    "# \n",
    "# 聚合方法size()和count()\n",
    "# size跟count的区别： size计数时包含NaN值，而count不包含NaN值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unstack用法\n",
    "# groupby（）可以根据DataFrame中的某一列或者多列内容进行分组聚合，\n",
    "# 当DataFrame聚合后为两列索引时，可以使用unstack()将聚合的两列中一列值调整为行索引，另一列的值调整为列索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({ 'col_1':['a', 'a', 'b', 'a', 'a', 'b', 'c', 'a', 'c'],\n",
    "                                      'col_2':['d', 'd', 'd', 'e', 'f', 'e', 'd', 'f', 'f'],\n",
    "                                      'col_3':[ 1,  2,  3,   1,  4,  5,  6,  4,  5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             col_3\ncol_1 col_2       \na     d          2\n      e          1\n      f          2\nb     d          1\n      e          1\nc     d          1\n      f          1\nMultiIndex(levels=[['a', 'b', 'c'], ['d', 'e', 'f']],\n           codes=[[0, 0, 0, 1, 1, 2, 2], [0, 1, 2, 0, 1, 0, 2]],\n           names=['col_1', 'col_2'])\nIndex(['col_3'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df1=test_df.groupby(['col_1', 'col_2']).count()\n",
    "print(df1)\n",
    "print(df1.index)\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      col_3          \ncol_2     d    e    f\ncol_1                \na       2.0  1.0  2.0\nb       1.0  1.0  NaN\nc       1.0  NaN  1.0\nIndex(['a', 'b', 'c'], dtype='object', name='col_1')\nMultiIndex(levels=[['col_3'], ['d', 'e', 'f']],\n           codes=[[0, 0, 0], [0, 1, 2]],\n           names=[None, 'col_2'])\n"
     ]
    }
   ],
   "source": [
    "# 对分组聚合后的数据进行unstack\n",
    "df1=test_df.groupby(['col_1', 'col_2']).count().unstack()\n",
    "print(df1)\n",
    "print(df1.index)\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_2    d    e    f\ncol_1               \na      2.0  1.0  2.0\nb      1.0  1.0  NaN\nc      1.0  NaN  1.0\nIndex(['a', 'b', 'c'], dtype='object', name='col_1')\nIndex(['d', 'e', 'f'], dtype='object', name='col_2')\n"
     ]
    }
   ],
   "source": [
    "# 对分组聚合后的某列进行unstack\n",
    "df1=test_df.groupby(['col_1', 'col_2']).count()['col_3'].unstack()\n",
    "print(df1)\n",
    "print(df1.index)\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
